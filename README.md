# Conversational RAG Chatbot with PDF Uploads

## ðŸš€ Overview
This project is a **Conversational Retrieval-Augmented Generation (RAG) Chatbot** built using **Streamlit**, **LangChain**, and **Groq's Llama 3 model**.  
It allows users to **upload PDF documents** (such as research papers) and engage in **context-aware conversations** with their content.  
By leveraging embeddings, vector databases, and chat memory, the chatbot provides accurate, grounded, and contextual answers to user queries.

---

## ðŸ§  Key Features
- ðŸ“š **PDF Uploads:** Upload and query multiple PDF documents.  
- ðŸ§© **Retrieval-Augmented Generation:** Combines semantic search with LLM reasoning.  
- ðŸ’¬ **Conversational Memory:** Maintains chat history for context across turns.  
- âš™ï¸ **Embeddings + Vector Search:** Uses HuggingFace embeddings and Chroma for fast retrieval.  
- âš¡ **Groq Llama 3 Model:** High-performance, low-latency language model for answering questions.  
- ðŸ§  **Session Management:** Supports multiple sessions with unique retrievers and chat histories.

---

## ðŸ› ï¸ Tech Stack
- **Python 3.10+**
- **Streamlit** â€“ Interactive UI
- **LangChain** â€“ Core orchestration framework
- **Groq (Llama 3.1-8b-instant)** â€“ Main LLM for text generation
- **HuggingFace Sentence Embeddings** â€“ For semantic vector representation
- **Chroma Vector Database** â€“ Vector storage and retrieval
- **PyPDFLoader** â€“ Extracts text from PDF files

---

## ðŸ“‚ Project Structure
```
.
â”œâ”€â”€ app.py                 # Main Streamlit app
â”œâ”€â”€ .env                   # Environment variables (API keys)
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ research_papers/       # (optional) Local directory for PDFs
â””â”€â”€ README.md              # Documentation
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/your-username/conversational-rag-chatbot.git
cd conversational-rag-chatbot
```

### 2ï¸âƒ£ Create and Activate a Virtual Environment
```bash
python -m venv venv
source venv/bin/activate   # macOS/Linux
venv\Scripts\activate    # Windows
```

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Configure Environment Variables
Create a `.env` file and add:
```
GROQ_API_KEY=your_groq_api_key
HF_TOKEN=your_huggingface_token
```

### 5ï¸âƒ£ Run the App
```bash
streamlit run app.py
```

---

## ðŸ’» Usage
1. Enter your **Groq API key** in the sidebar.  
2. **Upload one or more PDF files**.  
3. Wait for embeddings to build.  
4. Ask a question related to the uploaded documents.  
5. View real-time answers and chat history in the interface.

---

## ðŸ” How It Works
1. **PDF Ingestion:** Text extracted from uploaded PDFs.  
2. **Chunking:** Long texts are split into manageable segments.  
3. **Embeddings:** Text chunks are converted into vector embeddings.  
4. **Storage:** Embeddings are stored in a Chroma vector database.  
5. **Retrieval:** When queried, the most relevant document chunks are retrieved.  
6. **LLM Response:** Groqâ€™s Llama 3 generates an answer using the retrieved context.  
7. **Chat Memory:** History is saved for multi-turn, context-aware conversations.

---

## ðŸ§© Use Cases
- ðŸ§‘â€ðŸŽ“ **Academic Research:** Chat with your papers to summarize and explore concepts.  
- ðŸ¢ **Corporate Knowledge Base:** Query internal documents for instant insights.  
- ðŸ“š **Education:** Interactive learning from textbooks or notes.  

---

## ðŸŒ± Future Improvements
- ðŸ” Persistent Chroma storage across sessions  
- ðŸ”Š Voice-based input and output  
- ðŸŒ Multi-language document support  
- ðŸ§  Enhanced summarization and question generation  

---

## ðŸ‘©â€ðŸ’» Author
Developed by **[Your Name]**  
ðŸ“§ [your.email@example.com]  
ðŸ”— [LinkedIn / GitHub Profile]

---

## ðŸªª License
This project is licensed under the **MIT License**.  
Feel free to use and modify it with attribution.

---

### ðŸ’¬ Short Description
> A Streamlit-based conversational RAG chatbot that lets users upload PDFs and interact with them using Groqâ€™s Llama 3 model. It integrates retrieval, embeddings, and chat memory for accurate, context-aware responses.
